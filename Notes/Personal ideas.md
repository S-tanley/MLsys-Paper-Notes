1. 训练过程中进行类似于剪枝的操作，这样能有一个更小的模型，我不太知道蒸馏是怎么做的，但效果应该跟蒸馏类似。
2. 既然我们有分布式，MoE也有offloading，我们不如一个GPU上只存相关度比较高的experts，如果发现要用别的experts也不去load回来，而是把信息发给相对应的GPU。