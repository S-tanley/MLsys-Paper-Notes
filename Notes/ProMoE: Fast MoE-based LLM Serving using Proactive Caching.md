# ProMoE: Fast MoE-based LLM Serving using Proactive Caching[^1]

## Content

**Abstract**:

A novel proactive caching system that leverages intermediate model results to predict subsequent parameter usage. 

**1. Introduction**:

memory constraints.

**proactive** caching

 learned predictor, which is a neural network.

coordinate the execution of prefetching and inference processes to avoid them interfere each other

**9. Conclusion**

proactive caching approach

Mainly about cache but also involves prefetching.

less GPU, low inference latency.

## Notes



[^1]: Xiaoniu Song, Zihang Zhong, and Rong Chen. 2024. ProMoE: Fast MoE-based LLM Serving using Proactive Caching. [https://doi.org/10.48550/arXiv.2410.22134](https://urldefense.com/v3/__https:/doi.org/10.48550/arXiv.2410.22134__;!!Mak6IKo!JGsmunK4816F9qTIzFh0QAl_Tgr55CX5aR2IjjIRrsDaJvPzsI_qdQNouSpg-XfJEGex1dhRSBnoetB26mo3fg$)